{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VinDsl.jl: Fast and furious statistical modeling\n",
    " \n",
    "<img src=\"http://pearsonlab.github.io/images/plab_hex_icon_gray.png\" width=\"150\", align=\"left\", style=\"float:left\">\n",
    "\n",
    "<br>\n",
    "John Pearson  \n",
    "Duke Institute for Brain Sciences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About me\n",
    "\n",
    "<img src=\"http://pearsonlab.github.io/images/plab_logo_dark.svg\" width=\"400\">\n",
    "\n",
    "- computational neuroscience lab at Duke\n",
    "- using Julia for about a year\n",
    "- member of JuliaStats organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our problem  \n",
    "  \n",
    "<figure style=\"display:inline-block;font-size:50%;float:left\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/Monkey_grooming_02.JPG/640px-Monkey_grooming_02.JPG\" width=400 align=left>\n",
    "<figcaption>By AKS.9955, via Wikimedia Commons</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure style=\"display:inline-block;font-size:50%;float:right;text-align:center\">\n",
    "<img src=\"https://praneethnamburi.files.wordpress.com/2015/02/02_raster_baselineandstim.png\" width=400 align=right>\n",
    "<figcaption>https://praneethnamburi.wordpress.com/2015/02/05/simulating-neural-spike-trains/</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our problem\n",
    "\n",
    "- model responses to known features (like GLM)\n",
    "- infer latent features\n",
    "- do this scalably for large populations of neurons\n",
    "- use (generative) Bayesian models that account for uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# One solution: Sampling\n",
    "\n",
    "- Markov Chain Monte Carlo (MCMC) methods come with guarantees about correctness\n",
    "- lots of packages (Lora.jl, Mamba.jl, Stan.jl)\n",
    "- **But** sampling does not scale well to millions of observations and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Another solution: The Max Power Way\n",
    "\n",
    "<img src=\"max_power.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variational Bayesian Inference (VB)\n",
    "\n",
    "Generative model for data: $p(y|\\theta)p(\\theta)$  \n",
    "Actual posterior: $p(\\theta|y) = p(y|\\theta)p(\\theta)/p(y)$  \n",
    "Approximate posterior: $q(\\theta)$  \n",
    "\n",
    "Maximize **E**vidence **L**ower **Bo**und (ELBO) wrt $\\theta$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\log p(y) \\ge -KL\\left(q \\middle\\| p\\right) = \\mathbb{E}_q[\\log p(y|\\theta)] + \\mathcal{H}[q]\n",
    "$$\n",
    "\n",
    "- $\\mathbb{E}_q[\\log p(y|\\theta)]$ measures goodness of fit (data likely under approximate posterior)\n",
    "- $\\mathcal{H}[q]$ is the entropy (favors less certain models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why VB?\n",
    "\n",
    "- Scales well\n",
    "- Can use well-studied optimization techniques\n",
    "\n",
    "# Drawbacks:\n",
    "- !@$*&# hard to code\n",
    "- Can't quickly spec out a model like with Stan or JAGS/BUGS\n",
    "- Traditionally, requires that distributions be conjugate, requires doing lots of algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But VB is exploding!\n",
    "\n",
    "- stochastic variational inference (SVI): [Hoffman et al.](http://dl.acm.org/citation.cfm?id=2502622)\n",
    "- black box variational inference (BBVI): [Ranganath et al.](http://arxiv.org/abs/1401.0118)\n",
    "- control variates: [Paisley et al.](http://arxiv.org/abs/1206.6430)\n",
    "- local expectation gradients (LEG): [Titsias and L&aacute;zaro-Gredilla](http://papers.nips.cc/paper/5678-local-expectation-gradients-for-black-box-variational-inference)\n",
    "- neural variational inference (NVIL): [Mnih and Gregor](http://arxiv.org/abs/1402.0030)\n",
    "- variational autoencoders [Kingma and Welling](http://arxiv.org/abs/1312.6114), [Rezende et al.](http://arxiv.org/abs/1401.4082) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's out there\n",
    "\n",
    "- research code: individual algorithms, proof of concept \n",
    "- [Stan](http://mc-stan.org/) \n",
    "    - **Pros**: Robust, actively developed, good with stats, accessible from Julia\n",
    "    - **Cons**: variational methods still experimental, C++\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [Edward](https://github.com/blei-lab/edward)\n",
    "    - **Pros**: Python, multiple backends, from Stan and VB core developers\n",
    "    - **Cons**: Python, very new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's out there\n",
    "- [Theano](http://deeplearning.net/software/theano/) & [TensorFlow](https://www.tensorflow.org/)\n",
    "    - **Pros**: well-tested, stable, well-engineered backends\n",
    "    - **Cons**: complex, C++, not very stats-aware\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What do we want?\n",
    "- write math, get code &mdash; a domain-specific language (DSL)\n",
    "- easily generalize to different numbers of indices, structures\n",
    "- only weakly opinionated about model structure or inference\n",
    "- model code should be *hackable*\n",
    "    - easy to use prefab pieces\n",
    "    - not hard to write custom VB tricks\n",
    "    - fast prototyping\n",
    "- no (or minimal) algebra\n",
    "- automatic gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Introducing...\n",
    "![](http://www.joblo.com/newsimages1/vin.diesel_1920x1200_961)\n",
    "## VinDsl.jl: Fast and furious variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today: two prototypes\n",
    "- \"Classical\" models (conjugate + some optimization)\n",
    "- ADVI \"Black Box\" models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model 1\n",
    "\n",
    "### Finally, some code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition logpdf(Distributions.Gamma, Real) in module Distributions at /Users/jmxp/.julia/v0.4/Distributions/src/univariates.jl:321 overwritten in module VinDsl at /Users/jmxp/.julia/v0.4/VinDsl/src/distributions/expfam.jl:42.\n",
      "WARNING: Method definition logpdf(Distributions.Poisson, Int64) in module Distributions at /Users/jmxp/.julia/v0.4/Distributions/src/univariates.jl:321 overwritten in module VinDsl at /Users/jmxp/.julia/v0.4/VinDsl/src/distributions/expfam.jl:64.\n"
     ]
    }
   ],
   "source": [
    "using Distributions\n",
    "using VinDsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model structure:\n",
    "### Main idea: Factor graphs\n",
    "- idea from Dahua Lin in [this talk](http://people.csail.mit.edu/dhlin/jubayes/julia_bayes_inference.pdf)\n",
    "<img src=\"http://research.microsoft.com/en-us/um/people/cmbishop/prml/prmlfigs-png/Figure8.51.png\" width=\"200\">\n",
    "- Nodes: arrays of distributions\n",
    "- Factors $\\leftrightarrow$ terms in variational objective\n",
    "    - but not locked in to graphical model structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dims = (20, 6)\n",
    "\n",
    "μ[j] ~ Normal(zeros(dims[2]), ones(dims[2]))\n",
    "τ[j] ~ Gamma(1.1 * ones(dims[2]), ones(dims[2]))\n",
    "μ0[j] ~ Const(zeros(dims[2]))\n",
    "\n",
    "y[i, j] ~ Const(rand(dims));\n",
    "\n",
    "# SHOW METHOD HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Nodes: under the hood\n",
    "nodes define the q/approximate posterior/recognition model\n",
    "~ defines a node\n",
    "can use any distribution defined in the Distributions package\n",
    "code parses the left and right-hand sides\n",
    "indices on left get tracked and assigned to dimensions of parameter arrays\n",
    "code is rewritten as a call to a node constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f = @factor LogNormalFactor y μ τ;\n",
    "\n",
    "# SHOW METHOD HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the future..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@pmodel begin\n",
    "    y ~ Normal(μ, τ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "New factor types can be defined with yet another macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@deffactor LogNormalFactor [x, μ, τ] begin\n",
    "    -(1/2) * ((E(τ) * ( V(x) + V(μ) + (E(x) - E(μ))^2 ) + log(2π) + Elog(τ)))\n",
    "end\n",
    "\n",
    "@deffactor LogGammaFactor [x, α, β] begin\n",
    "    (E(α) - 1) * Elog(x) - E(β) * E(x) + E(α) * E(β) - Eloggamma(α)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uses a \"mini-language\" with `E(x)` $\\equiv \\mathbb{E}[X]$, `V(x)` $\\equiv \\textrm{cov}[X]$, etc.  \n",
    "- Again, no need to track indices\n",
    "    - multivariate distributions (Dirichlet, MvNormal) are automatically multivariate in these expressions\n",
    "- `VinDsl` generates a `value(f)` function that handles indices appropriately and sums over the dimensions of the array"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.4.5",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
