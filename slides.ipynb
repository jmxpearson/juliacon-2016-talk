{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VinDsl.jl: Fast and furious statistical modeling\n",
    " \n",
    "<img src=\"http://pearsonlab.github.io/images/plab_hex_icon_gray.png\" width=\"150\", align=\"left\", style=\"float:left\">\n",
    "\n",
    "<br>\n",
    "John Pearson  \n",
    "Duke Institute for Brain Sciences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About me\n",
    "\n",
    "<img src=\"http://pearsonlab.github.io/images/plab_logo_dark.svg\" width=\"400\">\n",
    "\n",
    "- computational neuroscience lab at Duke\n",
    "- using Julia for about a year\n",
    "- member of JuliaStats organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our problem  \n",
    "  \n",
    "<figure style=\"display:inline-block;font-size:50%;float:left\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/Monkey_grooming_02.JPG/640px-Monkey_grooming_02.JPG\" width=400 align=left>\n",
    "<figcaption>By AKS.9955, via Wikimedia Commons</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure style=\"display:inline-block;font-size:50%;float:right;text-align:center\">\n",
    "<img src=\"https://praneethnamburi.files.wordpress.com/2015/02/02_raster_baselineandstim.png\" width=400 align=right>\n",
    "<figcaption>https://praneethnamburi.wordpress.com/2015/02/05/simulating-neural-spike-trains/</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our problem\n",
    "\n",
    "- model responses to known features (like GLM)\n",
    "- infer latent features\n",
    "- do this scalably for large populations of neurons\n",
    "- use (generative) Bayesian models that account for uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# One solution: Sampling\n",
    "\n",
    "- Markov Chain Monte Carlo (MCMC) methods come with guarantees about correctness\n",
    "- lots of packages (Lora.jl, Mamba.jl, Stan.jl)\n",
    "- **But** sampling does not scale well to millions of observations and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Another solution: The Max Power Way\n",
    "\n",
    "<img src=\"max_power.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variational Bayesian Inference (VB)\n",
    "\n",
    "Generative model for data: $p(y|\\theta)p(\\theta)$  \n",
    "Actual posterior: $p(\\theta|y) = p(y|\\theta)p(\\theta)/p(y)$  \n",
    "Approximate posterior: $q(\\theta)$  \n",
    "\n",
    "Maximize **E**vidence **L**ower **Bo**und (ELBO) wrt $\\theta$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\log p(y) \\ge -KL\\left(q \\middle\\| p\\right) = \\mathbb{E}_q[\\log p(y|\\theta)] + \\mathcal{H}[q]\n",
    "$$\n",
    "\n",
    "- $\\mathbb{E}_q[\\log p(y|\\theta)]$ measures goodness of fit (data likely under approximate posterior)\n",
    "- $\\mathcal{H}[q]$ is the entropy (favors less certain models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why VB?\n",
    "\n",
    "- Scales well\n",
    "- Can use well-studied optimization techniques\n",
    "\n",
    "# Drawbacks:\n",
    "- !@$*&# hard to code\n",
    "- Can't quickly spec out a model like with Stan or JAGS/BUGS\n",
    "- Traditionally, requires that distributions be conjugate, requires doing lots of algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But VB is exploding!\n",
    "\n",
    "- stochastic variational inference (SVI): [Hoffman et al.](http://dl.acm.org/citation.cfm?id=2502622)\n",
    "- black box variational inference (BBVI): [Ranganath et al.](http://arxiv.org/abs/1401.0118)\n",
    "- control variates: [Paisley et al.](http://arxiv.org/abs/1206.6430)\n",
    "- local expectation gradients (LEG): [Titsias and L&aacute;zaro-Gredilla](http://papers.nips.cc/paper/5678-local-expectation-gradients-for-black-box-variational-inference)\n",
    "- neural variational inference (NVIL): [Mnih and Gregor](http://arxiv.org/abs/1402.0030)\n",
    "- variational autoencoders [Kingma and Welling](http://arxiv.org/abs/1312.6114), [Rezende et al.](http://arxiv.org/abs/1401.4082) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's out there\n",
    "\n",
    "- research code: individual algorithms, proof of concept \n",
    "- [Stan](http://mc-stan.org/) \n",
    "    - **Pros**: Robust, actively developed, good with stats, accessible from Julia\n",
    "    - **Cons**: variational methods still experimental, C++\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [Edward](https://github.com/blei-lab/edward)\n",
    "    - **Pros**: Python, multiple backends, from Stan and VB core developers\n",
    "    - **Cons**: Python, very new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's out there\n",
    "- [Theano](http://deeplearning.net/software/theano/) & [TensorFlow](https://www.tensorflow.org/)\n",
    "    - **Pros**: well-tested, stable, well-engineered backends\n",
    "    - **Cons**: complex, C++, not very stats-aware\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What do we want?\n",
    "- write math, get code &mdash; a domain-specific language (DSL)\n",
    "- easily generalize to different numbers of indices, structures\n",
    "- only weakly opinionated about model structure or inference\n",
    "- model code should be *hackable*\n",
    "    - easy to use prefab pieces\n",
    "    - not hard to write custom VB tricks\n",
    "    - fast prototyping\n",
    "- no (or minimal) algebra\n",
    "- automatic gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Introducing...\n",
    "![](http://www.joblo.com/newsimages1/vin.diesel_1920x1200_961)\n",
    "## VinDsl.jl: Fast and furious variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Philosophy\n",
    "- \"internal\" DSL\n",
    "- loosely-coupled parts\n",
    "- \"consenting adults\" outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Today: two prototypes\n",
    "- \"Classical\" models (conjugate + some optimization)\n",
    "- ADVI \"Black Box\" models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model 1\n",
    "\n",
    "### Finally, some code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition logpdf(Distributions.Gamma, Real) in module Distributions at /Users/jmxp/.julia/v0.4/Distributions/src/univariates.jl:321 overwritten in module VinDsl at /Users/jmxp/.julia/v0.4/VinDsl/src/distributions/expfam.jl:42.\n",
      "WARNING: Method definition logpdf(Distributions.Poisson, Int64) in module Distributions at /Users/jmxp/.julia/v0.4/Distributions/src/univariates.jl:321 overwritten in module VinDsl at /Users/jmxp/.julia/v0.4/VinDsl/src/distributions/expfam.jl:64.\n"
     ]
    }
   ],
   "source": [
    "using Distributions\n",
    "using VinDsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model structure:\n",
    "### Main idea: Factor graphs\n",
    "- idea from Dahua Lin in [this talk](http://people.csail.mit.edu/dhlin/jubayes/julia_bayes_inference.pdf)\n",
    "<img src=\"http://research.microsoft.com/en-us/um/people/cmbishop/prml/prmlfigs-png/Figure8.51.png\" width=\"200\">\n",
    "- Nodes: arrays of distributions\n",
    "- Factors $\\leftrightarrow$ terms in variational objective\n",
    "    - but not locked in to graphical model structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dims = (20, 6)\n",
    "\n",
    "μ[j] ~ Normal(zeros(dims[2]), ones(dims[2]))\n",
    "τ[j] ~ Gamma(1.1 * ones(dims[2]), ones(dims[2]))\n",
    "μ0[j] ~ Const(zeros(dims[2]))\n",
    "\n",
    "y[i, j] ~ Const(rand(dims));\n",
    "\n",
    "# SHOW METHOD HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Nodes: under the hood\n",
    "nodes define the q/approximate posterior/recognition model\n",
    "~ defines a node\n",
    "can use any distribution defined in the Distributions package\n",
    "code parses the left and right-hand sides\n",
    "indices on left get tracked and assigned to dimensions of parameter arrays\n",
    "code is rewritten as a call to a node constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f = @factor LogNormalFactor y μ τ;\n",
    "\n",
    "# SHOW METHOD HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the future..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@pmodel begin\n",
    "    y ~ Normal(μ, τ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "New factor types can be defined with yet another macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@deffactor LogNormalFactor [x, μ, τ] begin\n",
    "    -(1/2) * ((E(τ) * ( V(x) + V(μ) + (E(x) - E(μ))^2 ) + log(2π) + Elog(τ)))\n",
    "end\n",
    "\n",
    "@deffactor LogGammaFactor [x, α, β] begin\n",
    "    (E(α) - 1) * Elog(x) - E(β) * E(x) + E(α) * E(β) - Eloggamma(α)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uses a \"mini-language\" with `E(x)` $\\equiv \\mathbb{E}[X]$, `V(x)` $\\equiv \\textrm{cov}[X]$, etc.  \n",
    "- Again, no need to track indices\n",
    "    - multivariate distributions (Dirichlet, MvNormal) are automatically multivariate in these expressions\n",
    "- `VinDsl` generates a `value(f)` function that handles indices appropriately and sums over the dimensions of the array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Models are just factor graphs\n",
    "Let's do a simple conjugate model:  \n",
    "p-model:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j &\\sim \\mathcal{N}(0, 1) \\\\\n",
    "\\tau_j &\\sim \\mathrm{Gamma}(1.1, 1) \\\\\n",
    "y_{ij} &\\sim \\mathcal{N}(\\mu_j, \\tau_j)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "q-model:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu &\\sim \\mathcal{N}(m, t) \\\\\n",
    "\\tau &\\sim \\mathrm{Gamma}(\\alpha, \\beta) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dims = (20, 6)\n",
    "\n",
    "# note: it won't matter much how we initialize here\n",
    "μ[j] ~ Normal(zeros(dims[2]), ones(dims[2]))\n",
    "τ[j] ~ Gamma(1.1 * ones(dims[2]), ones(dims[2]))\n",
    "μ0[j] ~ Const(zeros(dims[2]))\n",
    "τ0[j] ~ Const(2 * ones(dims[2]))\n",
    "a0[j] ~ Const(1.1 * ones(dims[2]))\n",
    "b0[j] ~ Const(ones(dims[2]))\n",
    "\n",
    "y[i, j] ~ Const(rand(dims))\n",
    "\n",
    "# make factors\n",
    "obs = @factor LogNormalFactor y μ τ\n",
    "μ_prior = @factor LogNormalFactor μ μ0 τ0\n",
    "τ_prior = @factor LogGammaFactor τ a0 b0\n",
    "\n",
    "m = VBModel([μ, τ, μ0, τ0, a0, b0, y], [obs, μ_prior, τ_prior]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's going on here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update!(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{VinDsl.Node,Symbol} with 7 entries:\n",
       "  VinDsl.RandomNode{Distr… => :conjugate\n",
       "  VinDsl.ConstantNode{Flo… => :constant\n",
       "  VinDsl.RandomNode{Distr… => :conjugate\n",
       "  VinDsl.ConstantNode{Flo… => :constant\n",
       "  VinDsl.ConstantNode{Flo… => :constant\n",
       "  VinDsl.ConstantNode{Flo… => :constant\n",
       "  VinDsl.ConstantNode{Flo… => :constant"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.update_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conjugacy\n",
    "- right now `VinDsl` goes out of its way to handle conjugacy between nodes\n",
    "- conjugate relationships not automatically detected, but easy to define\n",
    "- `@defnaturals` returns expected sufficient statistics from a factor for a given target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@defnaturals LogNormalFactor μ Normal begin\n",
    "    Ex, Eτ = E(x), E(τ)\n",
    "    (Ex * Eτ, -Eτ/2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Index Bookkeeping\n",
    "- nodes have associated indices\n",
    "- factors know which indices go with which nodes, which indices to sum over\n",
    "    - inner indices belong to, e.g., elements of a multivariate normal (should not be separated)\n",
    "    - outer indices correspond to replicates of \"atomic\" variables  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So this is easy: `i` is inner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = 5\n",
    "μ[i] ~ MvNormalCanon(zeros(d), diagm(ones(d)))\n",
    "Λ[i, i] ~ Wishart(float(d), diagm(ones(d)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But here, `i` is inner for $\\mu$ but not for $\\tau$. In any factor combining these two, $\\tau$ will be treated like a vector because it matches an inner index for some node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "μ[i] ~ MvNormalCanon(zeros(d), diagm(ones(d)))\n",
    "τ[i] ~ Gamma(1.1 * ones(d), ones(d));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model 2: ADVI\n",
    "[(Automatic Differentiation Variational Inference)](http://arxiv.org/abs/1603.00788)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Two major ideas\n",
    "1. We just need to define an ELBO (or *approxmate* an ELBO)\n",
    "1. Any unimodal distribution is *approximately* a normal with constrained support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Implementation\n",
    "- Approximate ELBO by sampling from normal, transforming to constrained variables\n",
    "- Let automatic differentiation handle the gradient calculation\n",
    "- Do gradient ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hey, we have autodiff!\n",
    "![http://www.juliadiff.org/](juliadiff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But there's always a problem:\n",
    "- ForwardDiff defines `Dual <: Real`\n",
    "- But Distributions.jl doesn't allow, e.g., `Dual` $\\mu$ and $\\sigma$ for `Normal`\n",
    "- Then a lot of work..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today:\n",
    "- Some distributions allow arbitrary parameter types:\n",
    "    - using PDMats:\n",
    "        - MvNormal, MvNormalCanon, GenericMvTDist\n",
    "        - Wishart, InverseWishart\n",
    "    - other:\n",
    "        - Dirichlet, Normal, NormalCanon, Gamma, InverseGamma, Poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This month:\n",
    "- ~40 more distributions &mdash; almost all univariates\n",
    "- PR in progress from @Halmoni100 in my lab\n",
    "- some special functions in StatsFuns and Base still assume Float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Defining a simple model\n",
    "$$\\begin{align}\n",
    "\\sigma_\\eta &\\sim \\mathrm{Gamma}(a_\\eta, b_\\eta) \\\\\n",
    "a_u &\\sim \\mathcal{N}(\\mu_a, \\sigma_a) \\\\\n",
    "\\eta_{tu} &\\sim \\mathcal{N}(a_u, \\sigma_\\eta) \\\\\n",
    "N_{tu} &\\sim \\mathrm{Poisson}(e^{\\eta_{tu}}) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "function ELBO(x::Vector)\n",
    "    ctr = 1\n",
    "    L = zero(eltype(x))\n",
    "    \n",
    "    @advi_declarations begin\n",
    "        a::Real()[U]\n",
    "        σ::Positive()\n",
    "        η::Real[T, U]    \n",
    "    end\n",
    "    \n",
    "    @advi_model begin\n",
    "        for u in 1:U\n",
    "            a[u] ~ Normal(log(15.), 0.1)\n",
    "        end\n",
    "        S ~ InverseWishart(U, eye(U))\n",
    "        for t in 1:T\n",
    "            η[t] ~ MvNormal(a, S)\n",
    "        end\n",
    "        for t in 1:T\n",
    "            for u in 1:U\n",
    "                spikes[t, u] ~ Poisson(exp(η[t][u]))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    L\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
